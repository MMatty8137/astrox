{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdK0erdR92/7tOg+/SaQSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMatty8137/astrox/blob/stable/Roboflow_to_Coral_v1.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "vY8wAW5J5p_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This entire piece of a notebook is based on EdjeElectronics's notebook that can be seen [here](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite2_Object_Detction_Model.ipynb). Though everything is simplified and instructions are on how to make a Roboflow model work on any TFLite interpretor, **you do not need Google Coral for this notebook to be useful!**"
      ],
      "metadata": {
        "id": "hkzmgnW25JFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Getting data from Roboflow"
      ],
      "metadata": {
        "id": "yG0K8drM53YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the notebook, we will prepare a dataset on Roboflow and get all the necessary files for the model training."
      ],
      "metadata": {
        "id": "vZTV_JQW57Ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preparing the dataset\n",
        "\n",
        "In order to prepare a dataset on Roboflow you need to create an account, upload the files you want to annotate and annotate them. Roboflow has a pretty nice guide on how to go about that [here](https://blog.roboflow.com/getting-started-with-roboflow/)."
      ],
      "metadata": {
        "id": "D1sajEDc6Af4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Obtaining the necessary files\n",
        "\n",
        "You will need to click _deploy_ and click the big _Train Your Latest Dataset Version_, once that is done, open the _Versions_ and clock _Export_ button in the upper-right corner. We will need two exports, one in the _Pascal VOC_ file format, and the other in _TensorFlow TFRecord_ format, download both to your computer and unzip them into their respective subfolders.\n",
        "\n"
      ],
      "metadata": {
        "id": "NVikz5wm6hgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Preparing the necessary files\n",
        "\n",
        "### 1.3.1 images.zip\n",
        "First you will take the folders inside the Pascal VOC extracted folder, there should be _train_, _valid_ and _test_ and some readme's. Take the _train_, _valid_ and _test_ and zip them into a _images.zip_ file. Upload then that file to this notebook (drag&drop), it might take a while, this file weigh a few tens of mebibytes."
      ],
      "metadata": {
        "id": "lIkDNkGG7eF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 labelmap.txt\n",
        "\n",
        "Create an empty file called _labelmap.txt_ in the folder on your computer, keep the file open. Open the TFRecord extracted folder, open the _valid_ folder, open the _xxx_label_map.pbtxt_ file in Notepad and copy the contents. Paste the contents into your _labelmap.txt_ file. "
      ],
      "metadata": {
        "id": "uURSSv54764l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 labelmap.pbtxt\n",
        "\n",
        "Repeat the procedure, only change the result file name to _labelmap.pbtxt_, keep both labelmaps and paste them to this Jupyter notebook."
      ],
      "metadata": {
        "id": "IMFvy9oL8YMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.4 train.tfrecord and val.tfrecord\n",
        "\n",
        "Open the _train_ folder that is in the TFRecord file export. Find the file that ends in _.tfrecord_ and rename it to _train.tfrecord_. Repeat the procedure for _valid_ and rename to _val.tfrecord_. Upload both files to the Colab Jupyter notebook"
      ],
      "metadata": {
        "id": "K7sudT0D8wYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparing the bare TFLite model"
      ],
      "metadata": {
        "id": "pEs5v1OUExh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will prepare all the files necessary to train the pure model."
      ],
      "metadata": {
        "id": "aF6N3TkXE_vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "metadata": {
        "id": "lnJC_MGPFAFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy setup files into models/research folder\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "#cp object_detection/packages/tf2/setup.py ."
      ],
      "metadata": {
        "id": "GQJIthnoFDdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0\n",
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.8.0', s)\n",
        "    f.write(s)"
      ],
      "metadata": {
        "id": "ENe5pE46FFiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Object Detection API\n",
        "!pip install /content/models/research/\n",
        "\n",
        "# Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)\n",
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "4NvyB-B3FG3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model Bulider Test file, just to verify everything's working properly\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"
      ],
      "metadata": {
        "id": "j86daXiQFJhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to unzip our zip file with Pascal VOC data."
      ],
      "metadata": {
        "id": "6SBGi2l4FRkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q images.zip -d /content/images"
      ],
      "metadata": {
        "id": "D8uBDpVjFVWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we just define the files for further training."
      ],
      "metadata": {
        "id": "GyD0V3rLFaaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ],
      "metadata": {
        "id": "zAw5fpPbFczh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we choose the model and prepare the config."
      ],
      "metadata": {
        "id": "tOpBacHGFgg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
        "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    # The centernet model isn't working as of 9/10/22\n",
        "    #'centernet-mobilenet-v2': {\n",
        "    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n",
        "    #    'base_pipeline_file': 'pipeline.config',\n",
        "    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n",
        "    #}\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ],
      "metadata": {
        "id": "NcGxtsF2FlwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we get the model file."
      ],
      "metadata": {
        "id": "OPr6X-XqFobw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Download pre-trained model weights\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Download training configuration file for model\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ],
      "metadata": {
        "id": "zOuXupE5Fpoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to set the steps we will run, this is best figured out by trial and error, but more is always better at the cost of time. There are diminishing returns for anything more than 50000, also Colab may kill your process if it is way too long, so do not overdo it."
      ],
      "metadata": {
        "id": "RywhkV_-Fr7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 2500\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ],
      "metadata": {
        "id": "ZzC_qm-yF6ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we just check how many classes are detected. If getting any errors make sure that all the _pip_ cells up were activated, the libraries (obviously) depend on them."
      ],
      "metadata": {
        "id": "23RHMdRYF8e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)\n"
      ],
      "metadata": {
        "id": "WIs0Pwi7F_00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set the config file path for the model."
      ],
      "metadata": {
        "id": "187Wy3pQGp91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ],
      "metadata": {
        "id": "p0qjTgLDGt4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training the actual model"
      ],
      "metadata": {
        "id": "pwF0kVIgGxLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want, you can turn the TensorBoard on to see the pretty graphs!"
      ],
      "metadata": {
        "id": "TQmsGrmAHLan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ],
      "metadata": {
        "id": "4F1s3DgIHQpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important** The code below is actually the training code, make sure to **never** end it forcefully, do not close this window and make sure you do **not** get timeout, if you want to stop the training right-click into the field and end execution, do **not** use the stop button! "
      ],
      "metadata": {
        "id": "6-XrlEE2HSOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training!\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ],
      "metadata": {
        "id": "AVGQOVWZHjnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Convert to TFLite and TFLite for Coral"
      ],
      "metadata": {
        "id": "bQOZOMvpHpA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will prepare the actual TFLite model."
      ],
      "metadata": {
        "id": "Ii71WSfHH2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory to store the trained TFLite model\n",
        "!mkdir /content/custom_model_lite\n",
        "output_directory = '/content/custom_model_lite'\n",
        "\n",
        "# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}\n",
        "\n",
        "# Convert exported graph file into TFLite model file\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "b6TeIDXzHtud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also very useful to test the model before concluding it works, so here is the inferencing code that needs to be run before the actual testing."
      ],
      "metadata": {
        "id": "XjNNs0qbH-65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to run custom TFLite model on test images to detect objects\n",
        "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "### Define function for inferencing with TFLite model and displaying results\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "  # Grab filenames of all images in test folder\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  # Load the label map into memory\n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Load the Tensorflow Lite model into memory\n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get model details\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "  # Randomly select test images\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "  # Loop over every image and perform detection\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      # Load image and resize to expected shape [1xHxWx3]\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape \n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      # Perform the actual detection by running the model with the image as input\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      # Retrieve detection results\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
        "\n",
        "      detections = []\n",
        "\n",
        "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "              # Get bounding box coordinates and draw box\n",
        "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "              \n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # Draw label\n",
        "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "      \n",
        "      # All the results have been drawn on the image, now display the image\n",
        "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "      \n",
        "      # Save detection results in .txt files (for calculating mAP)\n",
        "      elif txt_only == True:\n",
        "\n",
        "        # Get filenames and paths\n",
        "        image_fn = os.path.basename(image_path)      \n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        # Write results to text file\n",
        "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "Mt3Vut1OIG2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this piece of code will do the actual testing! If you get any errors, check that the number of images you want to test is low enough, sometimes there is not enough _test_ images in the source folder. Also, if you do not get any results you may decrease the threshold, though that is not recommended as a model with too little accuracy will not be useful when used with real raw data."
      ],
      "metadata": {
        "id": "gaFkmGKxIMxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up variables for running user's model\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "min_conf_threshold=0.01   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 3   # Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "ybtucO2jIdne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next piece of code will prepare the model to be downloaded and download it (so far only the original TFLite model that will **not** run with the Coral, for that see below)"
      ],
      "metadata": {
        "id": "MliNjTLOIh3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/custom_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/custom_model_lite\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n",
        "\n",
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('/content/custom_model_lite.zip')"
      ],
      "metadata": {
        "id": "r-LfeWV_IqBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next piece of code with quantise the model and pack it for the Coral accelerator!"
      ],
      "metadata": {
        "id": "T_qxrjrwIz5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of all images in train directory\n",
        "image_path = '/content/images/train'\n",
        "\n",
        "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
        "JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
        "png_file_list = glob.glob(image_path + '/*.png')\n",
        "bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
        "\n",
        "quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"
      ],
      "metadata": {
        "id": "i3wbGYOJI6os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A generator that provides a representative dataset\n",
        "# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n",
        "\n",
        "# First, get input details for model so we know how to preprocess images\n",
        "interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "\n",
        "import random\n",
        "\n",
        "def representative_data_gen():\n",
        "  dataset_list = quant_image_list\n",
        "  quant_num = 300\n",
        "  for i in range(quant_num):\n",
        "    pick_me = random.choice(dataset_list)\n",
        "    image = tf.io.read_file(pick_me)\n",
        "\n",
        "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
        "      image = tf.io.decode_jpeg(image, channels=3)\n",
        "    elif pick_me.endswith('.png'):\n",
        "      image = tf.io.decode_png(image, channels=3)\n",
        "    elif pick_me.endswith('.bmp'):\n",
        "      image = tf.io.decode_bmp(image, channels=3)\n",
        "\n",
        "    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
        "    image = tf.cast(image / 255., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    yield [image]"
      ],
      "metadata": {
        "id": "gxvAHQjPI8Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize converter module\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input tensors to uint8 and output tensors to float32\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.float32\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "7jj2rE1BI964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test this model as well, it might be slightly worse, but way faster (you cannot tell that here apart though). Again, check how many images you are testing!"
      ],
      "metadata": {
        "id": "793egKWbI_9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n",
        "PATH_TO_IMAGES='/content/images/test'   #Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   #Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   #Path to labelmap.txt file\n",
        "min_conf_threshold=0.1   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 3   #Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "j4TanlutJGQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compile for the Edge TPU!"
      ],
      "metadata": {
        "id": "g7YQ8IsIJR09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "! sudo apt-get update\n",
        "! sudo apt-get install edgetpu-compiler\t\n",
        "\n",
        "# check the name! this one is the default but if you changed it, it will need to be changed here as well!\n",
        "%cd /content/custom_model_lite\n",
        "!edgetpu_compiler detect_quant.tflite\n",
        "!mv detect_quant_edgetpu.tflite edgetpu.tflite\n",
        "!rm detect_quant_edgetpu.log"
      ],
      "metadata": {
        "id": "yKNm8k2gJUFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can download!"
      ],
      "metadata": {
        "id": "gt--zCuOJeKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ],
      "metadata": {
        "id": "3uQ1O7raJfFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('custom_model_lite_for_coral.zip')"
      ],
      "metadata": {
        "id": "KMAHiYKoJfyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: How to deploy"
      ],
      "metadata": {
        "id": "mKHdoGv_Jwio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you may be asking how to deploy this model, I recommend reading [this](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/10a060a615155cafa781f1a974e1cb4b9489374e/deploy_guides/Windows_TFLite_Guide.md) guide for **Windows**. And [this](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/10a060a615155cafa781f1a974e1cb4b9489374e/deploy_guides/Raspberry_Pi_Guide.md) one for Raspberry. The links should be permalinks, but if they stop working please mail me at matyasmatta@email.cz."
      ],
      "metadata": {
        "id": "utDlfItnJ2D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, if you want to deploy via code and not via the command line I recommend the following code, credits to _someone_ who included the licence in the file but not their name, if anyone wants this code be removed please let me know, I do not want to upset anyone! "
      ],
      "metadata": {
        "id": "7Yxp0x2zKab1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the code, install the dependencies, connect the Coral (this is coral only) and set the labelmap and interpretor as seen in the ai_model function. Also set the image all the way down in the if function. The code should save the image as _meta.jpg_ and also the data in _ai_output.json_. Code is free to use for anyone! "
      ],
      "metadata": {
        "id": "qaq2E7qCLGjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "\n",
        "from pycoral.adapters import common\n",
        "from pycoral.adapters import detect\n",
        "from pycoral.utils.dataset import read_label_file\n",
        "from pycoral.utils.edgetpu import make_interpreter\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def draw_objects(draw, objs, labels):\n",
        "  \"\"\"Draws the bounding box and label for each object.\"\"\"\n",
        "  count = 0\n",
        "  for obj in objs:\n",
        "    bbox = obj.bbox\n",
        "    draw.rectangle([(bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax)],\n",
        "                   outline='red')\n",
        "    print(count)\n",
        "    draw.text((bbox.xmin + 10, bbox.ymin + 10),\n",
        "              '%s\\n%.2f' % (count, obj.score),\n",
        "              fill='red')\n",
        "    count += 1\n",
        "\n",
        "\n",
        "def ai_model(image_path):\n",
        "\n",
        "    open(r\"model\\labelmap.txt\")\n",
        "    labels = r'model\\labelmap.txt'\n",
        "    interpreter = make_interpreter(r'model\\edgetpu.tflite')\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    _, scale = common.set_resized_input(\n",
        "        interpreter, image.size, lambda size: image.resize(size, Image.ANTIALIAS))\n",
        "    print(scale)\n",
        "\n",
        "    # print('----INFERENCE TIME----')\n",
        "    # print('Note: The first inference is slow because it includes', 'loading the model into Edge TPU memory.')\n",
        "    for _ in range(2):\n",
        "        start = time.perf_counter()\n",
        "        interpreter.invoke()\n",
        "        inference_time = time.perf_counter() - start\n",
        "        objs = detect.get_objects(interpreter, 0, scale)\n",
        "        print('%.2f ms' % (inference_time * 1000))\n",
        "\n",
        "    # print('-------RESULTS--------')\n",
        "    if not objs:\n",
        "        print('No objects detected')\n",
        "    counter_for_ai_output = 0\n",
        "    ai_output = {}\n",
        "    for obj in objs:\n",
        "        #print(labels.get(obj.id, obj.id))\n",
        "        print('  id:    ', obj.id)\n",
        "        print('  score: ', obj.score)\n",
        "        print('  bbox:  ', obj.bbox)\n",
        "\n",
        "        # obj.bbox needs to be converted into a dictionary\n",
        "        bbox = obj.bbox\n",
        "        score = obj.score\n",
        "        ai_output[counter_for_ai_output] = {}\n",
        "        ai_output[counter_for_ai_output]['xmin'] = bbox.xmin\n",
        "        ai_output[counter_for_ai_output]['ymin'] = bbox.ymin\n",
        "        ai_output[counter_for_ai_output]['xmax'] = bbox.xmax\n",
        "        ai_output[counter_for_ai_output]['ymax'] = bbox.ymax\n",
        "        ai_output[counter_for_ai_output]['accuracy'] = score\n",
        "\n",
        "        counter_for_ai_output += 1\n",
        "    image = image.convert('RGB')\n",
        "    draw_objects(ImageDraw.Draw(image), objs, labels)\n",
        "    image.save('grace_hopper_processed.bmp')\n",
        "    \n",
        "        \n",
        "    # image.show()\n",
        "    if os.path.exists('meta.jpg') == True:\n",
        "        os.remove('meta.jpg')\n",
        "    image.save('meta.jpg')\n",
        "\n",
        "    with open('ai_output.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(ai_output, f, ensure_ascii=False, indent=4)\n",
        "    return ai_output\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = ai_model('### add some filepath here ###')\n",
        "    print(data)"
      ],
      "metadata": {
        "id": "BdaLvMkSK-x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: Common Errors"
      ],
      "metadata": {
        "id": "OOzFvK9YLk8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please add comments about the errors you have found!"
      ],
      "metadata": {
        "id": "_B2gmO73Lnwc"
      }
    }
  ]
}